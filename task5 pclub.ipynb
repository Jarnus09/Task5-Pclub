{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f297d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPer:\n",
    "    \n",
    "    def __init__(self, sizes, n_layers,  n_input, n_output,  activation, d_activation, opt,momentum, seed = 123):\n",
    "        #super().__init__()\n",
    "        self.sizes = sizes\n",
    "        self.n_layers = n_layers\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.activation = activation\n",
    "        self.d_activation = d_activation\n",
    "        self.weights_ = list()\n",
    "        self.biases_ = list()\n",
    "        self.weights_V = list()\n",
    "        self.biases_V = list()\n",
    "        self.opt = opt\n",
    "        self.momentum =  momentum\n",
    "        \n",
    "        \n",
    "        self.m_w = list()\n",
    "        self.v_w = list()\n",
    "        self.m_b = list()\n",
    "        self.v_b = list()\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "        self.t = 0\n",
    "        \n",
    "        w_temp = np.random.RandomState(seed)\n",
    "        self.weights_.append(w_temp.normal(loc=0.0, scale=0.1, size=(sizes[0], n_input)))\n",
    "        self.biases_.append(np.zeros(sizes[0]))\n",
    "        self.weights_V.append(np.zeros((sizes[0], n_input)))\n",
    "        self.biases_V.append(np.zeros(sizes[0]))\n",
    "        \n",
    "        self.m_w.append(np.zeros((sizes[0], n_input)))\n",
    "        self.v_w.append(np.zeros((sizes[0], n_input)))\n",
    "        self.m_b.append(np.zeros(sizes[0]))\n",
    "        self.v_b.append(np.zeros(sizes[0]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range (0,n_layers-1):\n",
    "            self.weights_.append(w_temp.normal(loc=0.0, scale=0.1, size=(sizes[i+1], sizes[i])))\n",
    "            self.biases_.append(np.zeros(sizes[i+1]))\n",
    "            self.weights_V.append(np.zeros((sizes[i+1], sizes[i])))\n",
    "            self.biases_V.append(np.zeros(sizes[i+1]))\n",
    "            \n",
    "            self.m_w.append(np.zeros((sizes[i+1], sizes[i])))\n",
    "            self.v_w.append(np.zeros((sizes[i+1], sizes[i])))\n",
    "            self.v_b.append(np.zeros(sizes[i+1]))\n",
    "            self.m_b.append(np.zeros(sizes[i+1]))\n",
    "            \n",
    "            \n",
    "        \n",
    "        self.weights_.append(w_temp.normal(loc=0.0, scale=0.1, size=(n_output, sizes[n_layers-1])))\n",
    "        self.biases_.append(np.zeros(n_output))\n",
    "        self.weights_V.append(np.zeros((n_output, sizes[n_layers-1])))\n",
    "        self.biases_V.append(np.zeros(n_output))\n",
    "        \n",
    "        self.m_w.append(np.zeros((n_output, sizes[n_layers-1])))\n",
    "        self.m_b.append(np.zeros(n_output))\n",
    "        self.v_w.append(np.zeros((n_output, sizes[n_layers-1])))\n",
    "        self.v_b.append(np.zeros(n_output))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forwardprop(self, X_train):\n",
    "        x = X_train\n",
    "        output_layer = list()\n",
    "        activation_layer = list()\n",
    "        for i in range (0,len(self.weights_)):\n",
    "            output_layer.append(np.dot(x, self.weights_[i].T) + self.biases_[i])\n",
    "            activation_layer.append(self.activation(output_layer[i]))\n",
    "            \n",
    "            x = activation_layer[i]\n",
    "            \n",
    "        return output_layer, activation_layer\n",
    "                                    \n",
    "    def backprop(self, X_train, y_train):\n",
    "        self.t+=1\n",
    "        output_layer, activation_layer = self.forwardprop(X_train)\n",
    "        y_onehot = int_to_onehot(y_train, self.n_output)\n",
    "        \n",
    "        grad_w = list()\n",
    "        grad_b = list()\n",
    "        loss_output = 2.*(activation_layer[-1] - y_onehot) / y_train.shape[0]\n",
    "        delta_out = loss_output * self.d_activation(activation_layer[-1])\n",
    "        \n",
    "        if(self.opt == 'adam'):\n",
    "               \n",
    "                \n",
    "                dW = (np.dot(delta_out.T, activation_layer[-2]))\n",
    "                db = (np.sum(delta_out, axis=0))\n",
    "                self.m_w[-1] = self.beta1 * self.m_w[-1] + (1 - self.beta1) * dW\n",
    "                self.m_b[-1] = self.beta1 * self.m_b[-1] + (1 - self.beta1) * db\n",
    "                self.v_w[-1] = self.beta2 * self.v_w[-1] + (1 - self.beta2) * (dW**2)\n",
    "                self.v_b[-1] = self.beta2 * self.v_b[-1] + (1 - self.beta2) * (db**2)\n",
    "                \n",
    "                m_W = self.m_w[-1] / (1 - self.beta1 ** self.t)\n",
    "                m_b = self.m_b[-1] / (1 - self.beta1 ** self.t)\n",
    "                \n",
    "                v_W = self.v_w[-1] / (1 - self.beta2 ** self.t)\n",
    "                v_b = self.v_b[-1] / (1 - self.beta2 ** self.t)\n",
    "                \n",
    "                grad_w.append(m_W / (np.sqrt(v_W) + self.epsilon))\n",
    "                grad_b.append(m_b / (np.sqrt(v_b) + self.epsilon))\n",
    "        else :                            \n",
    "                grad_w.append(np.dot(delta_out.T, activation_layer[-2]))\n",
    "                grad_b.append(np.sum(delta_out, axis=0))\n",
    "        \n",
    "        for i in range (1,len(output_layer)-1):\n",
    "             \n",
    "            if(self.opt=='nag'):\n",
    "                lookahead_W = self.weights_[-i] + self.momentum * self.weights_V[-i]\n",
    "                d_loss = np.dot(delta_out, lookahead_W)\n",
    "            else:\n",
    "                d_loss = np.dot(delta_out, self.weights_[-i])\n",
    " \n",
    "          \n",
    "            d_ah = self.d_activation(activation_layer[-i-1]) # sigmoid derivative\n",
    "   \n",
    "            d_z = activation_layer[-i-2]\n",
    "            \n",
    "            delta_out = d_loss*d_ah\n",
    "            \n",
    "            if(self.opt == 'adam'):\n",
    "               \n",
    "                \n",
    "                dW = np.dot((d_loss * d_ah).T, d_z)\n",
    "                db = np.sum((d_loss * d_ah), axis=0)\n",
    "                self.m_w[-i-1] = self.beta1 * self.m_w[-i-1] + (1 - self.beta1) * dW\n",
    "                self.m_b[-i-1] = self.beta1 * self.m_b[-i-1] + (1 - self.beta1) * db\n",
    "                self.v_w[-i-1] = self.beta2 * self.v_w[-i-1] + (1 - self.beta2) * (dW**2)\n",
    "                self.v_b[-i-1] = self.beta2 * self.v_b[-i-1] + (1 - self.beta2) * (db**2)\n",
    "                \n",
    "                m_W = self.m_w[-i-1] / (1 - self.beta1 ** self.t)\n",
    "                m_b = self.m_b[-i-1] / (1 - self.beta1 ** self.t)\n",
    "                \n",
    "                v_W = self.v_w[-i-1] / (1 - self.beta2 ** self.t)\n",
    "                v_b = self.v_b[-i-1] / (1 - self.beta2 ** self.t)\n",
    "                \n",
    "                grad_w.append(m_W / (np.sqrt(v_W) + self.epsilon))\n",
    "                grad_b.append(m_b / (np.sqrt(v_b) + self.epsilon))\n",
    "           \n",
    "            else:\n",
    "                \n",
    "                grad_w.append(np.dot((d_loss * d_ah).T, d_z))\n",
    "                grad_b.append(np.sum((d_loss * d_ah), axis=0))\n",
    "            \n",
    "        \n",
    "        if(self.opt=='nag'):\n",
    "                lookahead_W = self.weights_[-self.n_layers] + self.momentum * self.weights_V[-self.n_layers]\n",
    "                d_loss = np.dot(delta_out, lookahead_W)\n",
    "        else:\n",
    "                d_loss = np.dot(delta_out, self.weights_[-self.n_layers])\n",
    " \n",
    "  \n",
    "        d_ah = self.d_activation(activation_layer[-self.n_layers-1]) # sigmoid derivative\n",
    "  \n",
    "        d_z = X_train\n",
    "    \n",
    "        if(self.opt == 'adam'):\n",
    "               \n",
    "                \n",
    "                dW = np.dot((d_loss * d_ah).T, d_z)\n",
    "                db = np.sum((d_loss * d_ah), axis=0)\n",
    "                self.m_w[-self.n_layers-1] = self.beta1 * self.m_w[-self.n_layers-1] + (1 - self.beta1) * dW\n",
    "                self.m_b[-self.n_layers-1] = self.beta1 * self.m_b[-self.n_layers-1] + (1 - self.beta1) * db\n",
    "                self.v_w[-self.n_layers-1] = self.beta2 * self.v_w[-self.n_layers-1] + (1 - self.beta2) * (dW**2)\n",
    "                self.v_b[-self.n_layers-1] = self.beta2 * self.v_b[-self.n_layers-1] + (1 - self.beta2) * (db**2)\n",
    "                \n",
    "                m_W = self.m_w[-self.n_layers-1] / (1 - self.beta1 ** self.t)\n",
    "                m_b = self.m_b[-self.n_layers-1] / (1 - self.beta1 ** self.t)\n",
    "                \n",
    "                v_W = self.v_w[-self.n_layers-1] / (1 - self.beta2 ** self.t)\n",
    "                v_b = self.v_b[-self.n_layers-1] / (1 - self.beta2 ** self.t)\n",
    "                \n",
    "                grad_w.append(m_W / (np.sqrt(v_W) + self.epsilon))\n",
    "                grad_b.append(m_b / (np.sqrt(v_b) + self.epsilon))\n",
    " \n",
    "        else : \n",
    "                grad_w.append(np.dot((d_loss * d_ah).T, d_z))\n",
    "                grad_b.append(np.sum((d_loss * d_ah), axis=0))\n",
    "        \n",
    "        return grad_w, grad_b\n",
    "        \n",
    "     \n",
    "   \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1b5f7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_generator(X, y, minibatch_size):\n",
    "        indices = np.arange(X.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        for start_idx in range(0, indices.shape[0] - minibatch_size + 1, minibatch_size):\n",
    "            batch_idx = indices[start_idx:start_idx + minibatch_size]\n",
    "            yield X[batch_idx], y[batch_idx]\n",
    "\n",
    "def compute_loss_and_acc(model,X_train,y_train,  minibatch_size, p, num_labels = 10):\n",
    "    mse, correct_pred, num_examples = 0.,0,0\n",
    "    minibatch_gen = minibatch_generator(X_train,y_train, minibatch_size)\n",
    "    \n",
    "    net_loss = 0\n",
    "    for i, (features, targets) in enumerate(minibatch_gen):\n",
    "        __ , outputs = model.forwardprop(features)\n",
    "        predicted_output = np.argmax(outputs[-1],axis=1)\n",
    "        onehot_target = int_to_onehot(targets, num_labels=num_labels)\n",
    "        if(p==0):\n",
    "            loss = np.mean((onehot_target - outputs[-1])**2)\n",
    "        else :\n",
    "            outputs[-1] = softMax(outputs[-1])\n",
    "            loss = -np.mean(onehot_target*(np.log(outputs[-1])))\n",
    "        \n",
    "        correct_pred += (predicted_output==targets).sum()\n",
    "        num_examples +=targets.shape[0]\n",
    "        net_loss += loss\n",
    "    net_loss = net_loss/i\n",
    "    acc = correct_pred/num_examples\n",
    "    return net_loss,acc\n",
    "\n",
    "def train(model, X_train, y_train, X_valid, y_valid, num_epochs, learning_rate, minibatch_size, anneal,loss,momentum,opt):\n",
    "    if(loss == 'sq'):\n",
    "        p1 = 0\n",
    "    else :\n",
    "        p1 = 1\n",
    "    \n",
    "    epoch_loss = []\n",
    "    epoch_valid_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_valid_acc = []\n",
    "    \n",
    "    e = 0\n",
    "    while e<num_epochs:\n",
    "    # iterate over minibatches\n",
    "        minibatch_gen = minibatch_generator(X_train, y_train, minibatch_size)\n",
    "        for X_train_mini, y_train_mini in minibatch_gen:\n",
    "        #### Compute outputs ####\n",
    "       \n",
    "        #### Compute gradients ####\n",
    "            grad_w, grad_b = model.backprop(X_train_mini , y_train_mini)\n",
    "\n",
    "        #### Update weights ####\n",
    "            for i in range(len(model.weights_)):\n",
    "                if(opt == 'adam'):\n",
    "                    model.weights_[-i-1] -=learning_rate * grad_w[i]\n",
    "                    model.biases_[-i-1] -= learning_rate * grad_b[i]\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                \n",
    "                    model.weights_V[-i-1] = momentum * model.weights_V[-i-1] -learning_rate * grad_w[i]\n",
    "                    model.biases_V[-i-1] = momentum * model.biases_V[-i-1] - learning_rate * grad_b[i]\n",
    "                    model.weights_[-i-1] += model.weights_V[-i-1]\n",
    "                    model.biases_[-i-1] +=model.biases_V[-i-1]\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "        ### Epoch Logging ####\n",
    "        train_loss, train_acc = compute_loss_and_acc( model, X_train, y_train, minibatch_size, p = p1)\n",
    "        valid_loss, valid_acc = compute_loss_and_acc( model, X_valid, y_valid, minibatch_size, p = p1)\n",
    "        train_acc, valid_acc = train_acc*100, valid_acc*100\n",
    "        train_error = 100 - train_acc\n",
    "        valid_error = 100 -valid_acc\n",
    "        epoch_train_acc.append(train_acc)\n",
    "        epoch_valid_acc.append(valid_acc)\n",
    "        epoch_loss.append(train_loss)\n",
    "        epoch_valid_loss.append(valid_loss)\n",
    "        print(f'Epoch: {e+1:03d} 'f'| Loss: {valid_loss:.2f} 'f'| Error: {valid_error:.2f}% ' f'| lr: {learning_rate:.4f}')\n",
    "        if(anneal == True):\n",
    "            if(e>0):\n",
    "                if(valid_loss < epoch_valid_loss[-2]):\n",
    "                    learning_rate = learning_rate/2\n",
    "                    continue\n",
    "        e = e + 1\n",
    "       \n",
    "    return epoch_loss, epoch_train_acc, epoch_valid_acc, epoch_valid_loss\n",
    "\n",
    "def int_to_onehot(y, num_labels):\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for i, val in enumerate(y):\n",
    "         ary[i, val] = 1\n",
    "    return ary\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "def d_sigmoid(z):\n",
    "    return z*(1.-z)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def dTanh(z):\n",
    "    return 1/(np.cosh(z)**2)\n",
    "\n",
    "def softMax(X):\n",
    "    e = np.exp(X)\n",
    "    p = e/np.sum(e, axis=0)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2f440784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random as rd\n",
    "\n",
    "images = glob.glob('D:/cifar-10/train/*.png')\n",
    "train_labels = pd.read_csv('D:/cifar-10/trainLabels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe80b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['frog', 'truck', 'deer', 'automobile', 'bird', 'horse', 'ship', 'cat', 'dog', 'airplane']\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_val = []\n",
    "y_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51382484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "for img in images:\n",
    "    prob = rd.random()\n",
    "    label = train_labels.iloc[int(img[18:-4])-1]['label']\n",
    "    img_arr = cv2.imread(img)\n",
    "    img_arr = cv2.resize(img_arr, (32, 32))\n",
    "    if prob > 0.8:\n",
    "        X_val.append(list(img_arr))\n",
    "        y_val.append(LABELS.index(label))\n",
    "    else:\n",
    "        X_train.append(list(img_arr))\n",
    "        y_train.append(LABELS.index(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b94276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train, dtype=np.float32) / 255\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_val = np.array(X_val, dtype=np.float32) / 255\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e528fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train.reshape(X_train.shape[0],-1)\n",
    "X_train1.shape\n",
    "X_val1 = X_val.reshape(X_val.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b9b67e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if(args.activation == 'tanh'):\n",
    "    derivative = dTanh\n",
    "    activation = tanh\n",
    "    \n",
    "if(args.activation == 'sigmoid'):\n",
    "    derivative = d_sigmoid\n",
    "    activation = sigmoid\n",
    "\n",
    "model = MultiLayerPer(args.sizes,args.num_hidden,3072,10, activation, derivative, args.opt, args.momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "72f13498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Loss: 0.77 | Error: 88.31% | lr: 0.0100\n",
      "Epoch: 002 | Loss: 0.79 | Error: 86.27% | lr: 0.0100\n",
      "Epoch: 003 | Loss: 0.79 | Error: 86.67% | lr: 0.0100\n",
      "Epoch: 004 | Loss: 0.78 | Error: 85.90% | lr: 0.0100\n",
      "Epoch: 005 | Loss: 0.78 | Error: 87.28% | lr: 0.0100\n",
      "Epoch: 006 | Loss: 0.79 | Error: 90.30% | lr: 0.0100\n",
      "Epoch: 007 | Loss: 0.79 | Error: 89.24% | lr: 0.0100\n",
      "Epoch: 008 | Loss: 0.78 | Error: 89.61% | lr: 0.0100\n",
      "Epoch: 009 | Loss: 0.78 | Error: 88.00% | lr: 0.0100\n",
      "Epoch: 010 | Loss: 0.78 | Error: 86.70% | lr: 0.0100\n",
      "Epoch: 011 | Loss: 0.79 | Error: 87.16% | lr: 0.0100\n",
      "Epoch: 012 | Loss: 0.79 | Error: 90.09% | lr: 0.0100\n",
      "Epoch: 013 | Loss: 0.78 | Error: 87.47% | lr: 0.0100\n",
      "Epoch: 014 | Loss: 0.79 | Error: 89.08% | lr: 0.0100\n",
      "Epoch: 015 | Loss: 0.79 | Error: 88.14% | lr: 0.0100\n",
      "Epoch: 016 | Loss: 0.79 | Error: 88.02% | lr: 0.0100\n",
      "Epoch: 017 | Loss: 0.79 | Error: 88.18% | lr: 0.0100\n",
      "Epoch: 018 | Loss: 0.78 | Error: 86.94% | lr: 0.0100\n",
      "Epoch: 019 | Loss: 0.79 | Error: 87.71% | lr: 0.0100\n",
      "Epoch: 020 | Loss: 0.79 | Error: 88.79% | lr: 0.0100\n",
      "Epoch: 021 | Loss: 0.78 | Error: 87.92% | lr: 0.0100\n",
      "Epoch: 022 | Loss: 0.78 | Error: 88.60% | lr: 0.0100\n",
      "Epoch: 023 | Loss: 0.77 | Error: 87.87% | lr: 0.0100\n",
      "Epoch: 024 | Loss: 0.78 | Error: 87.89% | lr: 0.0100\n",
      "Epoch: 025 | Loss: 0.77 | Error: 88.35% | lr: 0.0100\n",
      "Epoch: 026 | Loss: 0.77 | Error: 88.07% | lr: 0.0100\n",
      "Epoch: 027 | Loss: 0.77 | Error: 87.76% | lr: 0.0100\n",
      "Epoch: 028 | Loss: 0.78 | Error: 87.40% | lr: 0.0100\n",
      "Epoch: 029 | Loss: 0.77 | Error: 86.41% | lr: 0.0100\n",
      "Epoch: 030 | Loss: 0.77 | Error: 84.85% | lr: 0.0100\n",
      "Epoch: 031 | Loss: 0.77 | Error: 86.09% | lr: 0.0100\n",
      "Epoch: 032 | Loss: 0.77 | Error: 85.02% | lr: 0.0100\n",
      "Epoch: 033 | Loss: 0.77 | Error: 86.29% | lr: 0.0100\n",
      "Epoch: 034 | Loss: 0.77 | Error: 87.38% | lr: 0.0100\n",
      "Epoch: 035 | Loss: 0.77 | Error: 85.09% | lr: 0.0100\n",
      "Epoch: 036 | Loss: 0.77 | Error: 85.22% | lr: 0.0100\n",
      "Epoch: 037 | Loss: 0.77 | Error: 85.20% | lr: 0.0100\n",
      "Epoch: 038 | Loss: 0.77 | Error: 83.29% | lr: 0.0100\n",
      "Epoch: 039 | Loss: 0.77 | Error: 85.81% | lr: 0.0100\n",
      "Epoch: 040 | Loss: 0.77 | Error: 85.94% | lr: 0.0100\n",
      "Epoch: 041 | Loss: 0.77 | Error: 85.61% | lr: 0.0100\n",
      "Epoch: 042 | Loss: 0.77 | Error: 86.59% | lr: 0.0100\n",
      "Epoch: 043 | Loss: 0.78 | Error: 86.78% | lr: 0.0100\n",
      "Epoch: 044 | Loss: 0.78 | Error: 87.92% | lr: 0.0100\n",
      "Epoch: 045 | Loss: 0.78 | Error: 87.47% | lr: 0.0100\n",
      "Epoch: 046 | Loss: 0.79 | Error: 88.12% | lr: 0.0100\n",
      "Epoch: 047 | Loss: 0.78 | Error: 87.53% | lr: 0.0100\n",
      "Epoch: 048 | Loss: 0.77 | Error: 87.86% | lr: 0.0100\n",
      "Epoch: 049 | Loss: 0.77 | Error: 87.77% | lr: 0.0100\n",
      "Epoch: 050 | Loss: 0.77 | Error: 86.81% | lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "epoch_loss, epoch_train_acc, epoch_valid_acc, epoch_valid_loss = train(model, X_train1, y_train, X_val1, y_val, 50 ,args.lr,args.minibatch_size,args.anneal, args.opt,args.momentum,args.opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ba230412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --lr LR --momentum MOMENTUM --num_hidden NUM_HIDDEN --sizes SIZES --activation\n",
      "                             ACTIVATION --loss LOSS --opt OPT --batch_size BATCH_SIZE --anneal ANNEAL --save_dir\n",
      "                             SAVE_DIR --expt_dir EXPT_DIR --train TRAIN --test TEST\n",
      "ipykernel_launcher.py: error: the following arguments are required: --lr, --momentum, --num_hidden, --sizes, --activation, --loss, --opt, --batch_size, --anneal, --save_dir, --expt_dir, --train, --test\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "def comma_separated_list(string):\n",
    "    \n",
    "    items = [item.strip() for item in string.split(',')]\n",
    "    return items\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lr', type=int, required=True)\n",
    "parser.add_argument('--momentum', type=int, required=True)\n",
    "parser.add_argument('--num_hidden', type=int, required=True)\n",
    "parser.add_argument('--sizes', type=comma_separated_list, required=True)\n",
    "parser.add_argument('--activation', type=str, required=True)\n",
    "parser.add_argument('--loss', type=str, required=True)\n",
    "parser.add_argument('--opt', type=str, required=True)\n",
    "parser.add_argument('--batch_size', type=str, required=True)\n",
    "parser.add_argument('--anneal', type=bool, required=True)\n",
    "parser.add_argument('--save_dir', type=str, required=True)\n",
    "parser.add_argument('--expt_dir', type=str, required=True)\n",
    "parser.add_argument('--train', type=str, required=True)\n",
    "parser.add_argument('--test', type=str, required=True)\n",
    "args = parser.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a6a9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
